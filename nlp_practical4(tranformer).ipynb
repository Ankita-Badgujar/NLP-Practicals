{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLcHaoLwyd4J"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB1cjjCgyhj0",
        "outputId": "d2013f5d-7a14-453a-c1d2-3badafa3eb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 8\n",
        "embed_len = 512\n",
        "batch_size = 8              # chosen batch size\n",
        "stack_len = 6               # length of encoder and decoder stacks (=6 as used in paper)\n",
        "dropout = 0.1               # dropout value to use\n",
        "\n",
        "output_vocab_size = 7000    # just a dummy number\n",
        "input_vocab_size = 7000     # just a dummy number"
      ],
      "metadata": {
        "id": "wKASF9DHyiOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, input_vocab_size=input_vocab_size, embed_len=embed_len, dropout=dropout, device=device):\n",
        "        super(InputEmbedding, self).__init__()\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.embed_len = embed_len\n",
        "        self.device = device\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.firstEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
        "        self.secondEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
        "\n",
        "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def forward(self, input):\n",
        "        first_embedding = self.firstEmbedding(input)\n",
        "        \n",
        "        batch_size, seq_len = input.shape\n",
        "\n",
        "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(self.device)\n",
        "        second_embedding = self.secondEmbedding(positions_vector)\n",
        "\n",
        "        return self.dropoutLayer(first_embedding + second_embedding)"
      ],
      "metadata": {
        "id": "2PUuSOuNyr91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProduct(nn.Module):\n",
        "    def __init__(self, embed_len=embed_len, mask=None):\n",
        "        super(ScaledDotProduct, self).__init__()\n",
        "        \n",
        "        self.dk = embed_len                 # dk = embed_len\n",
        "        self.mask = mask\n",
        "        self.softmax = nn.Softmax(dim=3)    # Softmax operator\n",
        "\n",
        "    # Define the forward function\n",
        "    def forward(self, queries, keys, values):       \n",
        "\n",
        "        # First batch MatMul operation & scaling down by sqrt(dk).\n",
        "        # Output 'compatibility' has shape:\n",
        "        # (batch_size, num_heads, seq_len, seq_len)\n",
        "        compatibility = torch.matmul(queries, torch.transpose(keys, 2, 3)) \n",
        "        compatibility = compatibility / math.sqrt((self.dk))               \n",
        "\n",
        "        # Apply mask after scaling the result of MatMul of Q and K.\n",
        "        # This is needed in the decoder to prevent the decoder from\n",
        "        # 'peaking ahead' and knowing what word will come next.\n",
        "        # Check: https://pytorch.org/docs/stable/generated/torch.tril.html \n",
        "        if self.mask is not None:\n",
        "            compatibility = torch.tril(compatibility)\n",
        "            \n",
        "        # Normalize using Softmax\n",
        "        compatibility_softmax = self.softmax(compatibility)        \n",
        "               \n",
        "        return torch.matmul(compatibility_softmax, torch.transpose(values, 1, 2))"
      ],
      "metadata": {
        "id": "U8qBSkwoyswz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anEp31HkyvnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads=num_heads, embed_len=embed_len, batch_size=batch_size, mask=None):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.batch_size = batch_size\n",
        "        self.embed_len = embed_len\n",
        "        self.head_length = int(self.embed_len/self.num_heads)\n",
        "        self.mask = mask\n",
        "        self.concat_output = []\n",
        "\n",
        "        # Q, K, and V have shape: (batch_size, seq_len, embed_len)\n",
        "        self.q_in = self.k_in = self.v_in = self.embed_len\n",
        "\n",
        "        # Linear layers take in embed_len as input \n",
        "        # dim and produce embed_len as output dim\n",
        "        self.q_linear = nn.Linear(int(self.q_in), int(self.q_in))\n",
        "        self.k_linear = nn.Linear(int(self.k_in), int(self.k_in))\n",
        "        self.v_linear = nn.Linear(int(self.v_in), int(self.v_in))\n",
        "\n",
        "        # Attention layer.\n",
        "        if self.mask is not None:\n",
        "            self.attention = ScaledDotProduct(mask=True) \n",
        "        else:\n",
        "            self.attention = ScaledDotProduct()\n",
        "\n",
        "        self.output_linear = nn.Linear(self.q_in, self.embed_len)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "\n",
        "        # Query has shape: (batch_size, seq_len, num_heads, head_length)\n",
        "        # Then transpose it: (batch_size, num_heads, seq_len, head_length)\n",
        "        queries = self.q_linear(queries).reshape(\n",
        "            self.batch_size, -1, self.num_heads, self.head_length)\n",
        "        queries = queries.transpose(1, 2)\n",
        "\n",
        "        # Same for Key as for Query above.\n",
        "        keys = self.k_linear(keys).reshape(\n",
        "            self.batch_size, -1, self.num_heads, self.head_length)\n",
        "        keys = keys.transpose(1, 2)\n",
        "\n",
        "        # Value has shape: (batch_size, seq_len, num_heads, head_length)\n",
        "        values = self.v_linear(values).reshape(\n",
        "            self.batch_size, -1, self.num_heads, self.head_length)\n",
        "\n",
        "        # 'sdp_output' here has size: \n",
        "        # (batch_size, num_heads, seq_len, head_length)\n",
        "        sdp_output = self.attention.forward(queries, keys, values)\n",
        "\n",
        "        # Reshape to (batch_size, seq_len, num_heads*head_length)\n",
        "        sdp_output = sdp_output.transpose(1, 2).reshape(\n",
        "            self.batch_size, -1, self.num_heads * self.head_length)\n",
        "\n",
        "        # Return self.output_linear(sdp_output).\n",
        "        # This has shape (batch_size, seq_len, embed_len)\n",
        "        return self.output_linear(sdp_output)"
      ],
      "metadata": {
        "id": "MmpXwlfFyx2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self.embed_len = embed_len\n",
        "        self.dropout = dropout\n",
        "        self.multihead = MultiHeadAttention()             # Multi-Head Attention layer\n",
        "        self.firstNorm = nn.LayerNorm(embed_len)          # Normalization layer (after the multi-head attention layer)\n",
        "        self.secondNorm = nn.LayerNorm(embed_len)         # Normalization layer (after the Feed Forward layer)\n",
        "        self.dropoutLayer = nn.Dropout(p=self.dropout)    # Dropout layer (before addition and normalization)\n",
        "\n",
        "        # The Feed Forward layer. In the paper this has input &\n",
        "        # output = 512 (or = embed_len) and inner-layer = 2048 (or = embed_len*4)\n",
        "        self.feedForward = nn.Sequential(\n",
        "            nn.Linear(embed_len, embed_len*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_len*4, embed_len)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        attention_output = self.multihead.forward(queries, keys, values)\n",
        "        attention_output = self.dropoutLayer(attention_output)\n",
        "\n",
        "        # the output of the first residual connection\n",
        "        first_sublayer_output = self.firstNorm(attention_output + queries)\n",
        "\n",
        "        ff_output = self.feedForward(first_sublayer_output)\n",
        "        ff_output = self.dropoutLayer(ff_output)\n",
        "\n",
        "        # return the output of the second residual connection\n",
        "        return self.secondNorm(ff_output + first_sublayer_output)"
      ],
      "metadata": {
        "id": "1SSrdW0XyysT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_len=embed_len, dropout=dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.embed_len = embed_len\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Masked Multi-Head Attention and Normalization layers.\n",
        "        self.maskedMultihead = MultiHeadAttention(mask=True)\n",
        "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
        "\n",
        "        self.dropoutLayer = nn.Dropout(p=self.dropout)\n",
        "\n",
        "        # The output of the above two layers and the output from the encoder stack feed \n",
        "        # into an 'encoder block'\n",
        "        self.encoderBlock = EncoderBlock()\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "\n",
        "        # First sublayer, which consists of the Masked Multi-Head Attention + Normalization\n",
        "        # sublayer, with a residual connection\n",
        "        masked_multihead_output = self.maskedMultihead.forward(queries, queries, queries)\n",
        "        masked_multihead_output = self.dropoutLayer(masked_multihead_output)\n",
        "        first_sublayer_output = self.firstNorm(masked_multihead_output + queries)\n",
        "\n",
        "        # The remaining of the DecoderBlock is basically an encoder block, which takes keys \n",
        "        # and values from the actual Encoder stack output, and takes queries from the \n",
        "        # previous sublayer of the DecoderBlock\n",
        "        return self.encoderBlock.forward(first_sublayer_output, keys, values)      "
      ],
      "metadata": {
        "id": "shuvzvjPy21m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, stack_len=stack_len, embed_len=embed_len, device=device, output_vocab_size=output_vocab_size):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.stack_len = stack_len\n",
        "        self.embed_len = embed_len\n",
        "        self.device = device\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "\n",
        "        self.embedding = InputEmbedding().to(self.device)\n",
        "        self.encStack = nn.ModuleList([EncoderBlock() for i in range(self.stack_len)])\n",
        "        self.decStack = nn.ModuleList([DecoderBlock() for i in range(self.stack_len)])\n",
        "        self.finalLinear = nn.Linear(self.embed_len, self.output_vocab_size)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, test_input, test_target):\n",
        "\n",
        "        enc_output = self.embedding.forward(test_input)\n",
        "\n",
        "        # Final output 'enc_output' of this loop will be both the key and value\n",
        "        # that will be taken as input to the second sub-layer of the decoder\n",
        "        for enc_layer in self.encStack:\n",
        "            enc_output = enc_layer.forward(enc_output, enc_output, enc_output)\n",
        "\n",
        "        # Decoder stack will take the 'enc_output' from the decoder as the keys\n",
        "        # and values, and will take its own output from the previous layer as\n",
        "        # the query. The query used for the first layer is the '<sos>' token.\n",
        "        dec_output = self.embedding(test_target)\n",
        "        for dec_layer in self.decStack:\n",
        "            dec_output = dec_layer.forward(dec_output, enc_output, enc_output)\n",
        "\n",
        "        # Pass the final decoder stack output to the linear layer that takes in\n",
        "        # input vector of size 'embed_len' and outputs a vector that has the \n",
        "        # size of the vocab specified. Finall return the softmax output of that vector\n",
        "        final_output = self.finalLinear(dec_output)\n",
        "\n",
        "        return self.softmax(final_output)"
      ],
      "metadata": {
        "id": "OTjKv7Kyy5f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = torch.randint(10, (batch_size, 30)).to(device)\n",
        "output_target = torch.randint(10, (batch_size, 20)).to(device)\n",
        "\n",
        "Embedding = InputEmbedding().to(device)\n",
        "input_embeddings = Embedding.forward(input_tokens).to(device)\n",
        "\n",
        "transformer = Transformer().to(device)\n",
        "print(input_embeddings.shape)\n",
        "\n",
        "transformer_output = transformer.forward(input_tokens, output_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERNxH8Vcy8iH",
        "outputId": "17abd5eb-6c6b-4505-ebae-671cfbc1475d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 30, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-ed370a3eed6a>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.softmax(final_output)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformer_output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK1eTWo3y-8h",
        "outputId": "2512b754-fb08-462c-def6-06d9abe3f95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 20, 7000])\n"
          ]
        }
      ]
    }
  ]
}